{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOos3nzGaBImR59oLZ2x/UH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ehtisham1053/Natural-Language-Processing/blob/main/Term_in_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Corpus\n",
        "A corpus (plural: corpora) is a large and structured collection of text data used for training NLP models. It can consist of sentences, paragraphs, or entire documents and is often used for linguistic analysis, machine learning, and deep learning applications.\n",
        "\n",
        "#ðŸ”¹ Example:\n",
        "\n",
        "SNLI Corpus\n",
        "* â€“ A dataset for natural language inference.\n",
        "Wikipedia Corpus\n",
        "* â€“ A collection of articles from Wikipedia used for training language models like BERT.\n",
        "####ðŸ”¹ Code Example (Loading Corpus using NLTK):"
      ],
      "metadata": {
        "id": "lKEWH0YwSVXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ22msVQM5Vo",
        "outputId": "ffe166f1-323a-4f1b-ed82-39d72a0418d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('reuters')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample corpus (Reuters dataset)\n",
        "corpus = reuters.sents()\n",
        "print(corpus[:2])  # First two sentences in tokenized form"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL768loIS5Nb",
        "outputId": "038661a0-e80f-4909-bfe3-d256f53050b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vocabulary\n",
        "A vocabulary is the set of unique words present in a given corpus. It represents all the words that an NLP model can recognize and process. The vocabulary size can impact the complexity and efficiency of NLP models.\n",
        "\n",
        "##ðŸ”¹ Example:\n",
        "If the corpus contains these three sentences:\n",
        "\n",
        "* \"I love NLP.\"\n",
        "* \"NLP is amazing.\"\n",
        "* \"I love deep learning.\"\n",
        "\n",
        "The vocabulary will be:\n",
        "* {\"I\", \"love\", \"NLP\", \"is\", \"amazing\", \"deep\", \"learning\"} (7 unique words)"
      ],
      "metadata": {
        "id": "gZbOJunJThid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\"I love NLP NLP is amazing I love deep learning\"]\n",
        "\n",
        "# Create vocabulary\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# Get vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VpoK-_qSm7E",
        "outputId": "738f07db-8616-4ce9-c5e2-578e324770a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amazing' 'deep' 'is' 'learning' 'love' 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document\n",
        "A document in NLP refers to a single text unit within a corpus. It can be a sentence, paragraph, or even an entire article, depending on the problem being solved.\n",
        "\n",
        "## ðŸ”¹ Example:\n",
        "In a news classification problem, each news article is considered a document.\n",
        "If we have the following dataset:\n",
        "\n",
        "* Document 1: \"The stock market is rising today.\"\n",
        "* Document 2: \"New advancements in AI are transforming the world.\""
      ],
      "metadata": {
        "id": "KMF6alA4UOMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "document = \"Natural Language Processing is a fascinating field. And this is the lecture for the NLP\"\n",
        "tokens = sent_tokenize(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPsye0jiTud8",
        "outputId": "d6bdc5ad-6603-4408-eb63-1ef2418c878c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnQ6CFgZU4HA",
        "outputId": "7f502208-f15c-4326-8877-2141e1c23be4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural Language Processing is a fascinating field.', 'And this is the lecture for the NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word\n",
        "A word is the smallest meaningful unit in a document. In NLP, words are processed using tokenization techniques to extract features and train models.\n",
        "\n",
        "\n",
        "##ðŸ”¹ Example:\n",
        "For the sentence:\n",
        "\"AI is revolutionizing industries.\"\n",
        "The words are: [\"AI\", \"is\", \"revolutionizing\", \"industries\"]"
      ],
      "metadata": {
        "id": "W7vqTU9-Ui_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "document = \"Natural Language Processing is a fascinating field.\"\n",
        "tokens = word_tokenize(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiR8ehBQUX9z",
        "outputId": "49cfa62f-a161-4390-a34c-bb00de3ad3c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjGtb-r2U_5F",
        "outputId": "d69cf4f5-b000-4c9e-9c06-63aab6b12e20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yysMbQq3VCm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}