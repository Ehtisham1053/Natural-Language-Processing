{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNurMbHjSfbvPPFHXcbXYPi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ehtisham1053/Natural-Language-Processing/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1Ô∏è‚É£ What is Tokenization?\n",
        "Tokenization is the process of splitting text into smaller units, called tokens. These tokens can be words, subwords, sentences, or characters.\n",
        "\n",
        "##üìå Example\n",
        "* Input Text:\n",
        "üëâ \"Natural Language Processing is amazing!\"\n",
        "\n",
        "* After Tokenization (Word-based)\n",
        "üëâ [\"Natural\", \"Language\", \"Processing\", \"is\", \"amazing\", \"!\"]"
      ],
      "metadata": {
        "id": "__0mZqlJq-io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2Ô∏è‚É£ Why Use Tokenization?\n",
        "Tokenization is a fundamental step in NLP because:\n",
        "\n",
        "* Standardizes input data (splitting text into meaningful parts).\n",
        "* Prepares text for further processing (vectorization, embeddings, etc.).\n",
        "* Helps models understand structure (sentences, words, etc.).\n",
        "* Reduces computational complexity by breaking down large text into smaller parts."
      ],
      "metadata": {
        "id": "88AQkVPWrFWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3Ô∏è‚É£ When to Use Tokenization?\n",
        "Tokenization is used in various NLP tasks, such as: ‚úÖ Preprocessing for Machine Learning Models (Text Classification, Sentiment Analysis, etc.)\n",
        "* ‚úÖ Information Retrieval & Search Engines (Splitting text into searchable units)\n",
        "* ‚úÖ Chatbots & Conversational AI (Understanding input sentences)\n",
        "* ‚úÖ Text Summarization & Translation (Breaking text into manageable parts)\n",
        "* ‚úÖ Speech-to-Text Processing (Segmenting spoken text into meaningful words)"
      ],
      "metadata": {
        "id": "1N9iuMourN7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4Ô∏è‚É£ Types of Tokenization"
      ],
      "metadata": {
        "id": "aih7MXJ-rUzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 1. Word Tokenization"
      ],
      "metadata": {
        "id": "RV2-60HprW7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcPWzomHpZl2",
        "outputId": "ee7dd652-96e4-4dba-ca78-b5a723e35d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"I love NLP! It's amazing.\"\n",
        "tokens = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"original text\" , text)\n",
        "print(\"tokens\" , tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iwMFqDyrZ0q",
        "outputId": "6ff63f11-c388-4f2c-bfc5-697a15fcc574"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text I love NLP! It's amazing.\n",
            "tokens ['I', 'love', 'NLP', '!', 'It', \"'s\", 'amazing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Challenges:\n",
        "\n",
        "*  Doesn't handle contractions well (It's ‚Üí ['It', \"'s\"]).\n",
        "*  Punctuation is treated as separate tokens."
      ],
      "metadata": {
        "id": "y0QL7X1jsXU_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXmk0pd5rtV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 2. Sentence Tokenization"
      ],
      "metadata": {
        "id": "sGmBORl1sco2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"NLP is great. It helps machines understand language!\"\n",
        "sentences = sent_tokenize(text)\n"
      ],
      "metadata": {
        "id": "tAsoS_JNsdZR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrsBS6Q5sf3n",
        "outputId": "9619ddbf-28a3-42ee-abdb-1d1a1215c5b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP is great.', 'It helps machines understand language!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Challenges:\n",
        "\n",
        "* Hard to distinguish abbreviations (Dr. Smith is here. vs Dr. Smith is here).\n",
        "* Some languages don‚Äôt use punctuation to separate sentences."
      ],
      "metadata": {
        "id": "YSs7-It7sjxn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-EnuLbleshRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 3. Character Tokenization"
      ],
      "metadata": {
        "id": "GllQOdUYsm65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello!\"\n",
        "tokens = list(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoEeysU5snph",
        "outputId": "a74523e1-b32f-4ca8-8729-82dce2c2f3b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'e', 'l', 'l', 'o', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Use Case:\n",
        "\n",
        "* Used in models like RNNs and Transformer-based models that process individual characters.\n",
        "* Useful in OCR (Optical Character Recognition) tasks."
      ],
      "metadata": {
        "id": "UnfQ6pxlsrLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MA8kwAJspKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 4. Subword Tokenization (Byte Pair Encoding - BPE)\n",
        "Instead of splitting at spaces, it breaks words into smaller subwords.\n",
        "* ‚úÖ Handles rare words better.\n",
        "* ‚úÖ Used in transformer-based models (BERT, GPT, etc.)"
      ],
      "metadata": {
        "id": "i7V5UwzMsvFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. \"unhappiness\" ‚Üí [\"un\", \"happiness\"]\n",
        "3. \"playing\" ‚Üí [\"play\", \"ing\"]\n"
      ],
      "metadata": {
        "id": "7vZLM7VFs27n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bpemb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXa00RHktOaZ",
        "outputId": "da83aba5-fa53-4cc7-aa83-c6c0779cfa11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.6-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting gensim (from bpemb)\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bpemb) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bpemb) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from bpemb) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from bpemb) (4.67.1)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim->bpemb)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->bpemb) (7.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bpemb) (2025.1.31)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->bpemb) (1.17.2)\n",
            "Downloading bpemb-0.3.6-py3-none-any.whl (20 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim, bpemb\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed bpemb-0.3.6 gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "bpemb_en = BPEmb(lang=\"en\", vs=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39bMvQ53tS1X",
        "outputId": "2434afea-bf21-426d-da85-9ae022aa0f14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400869/400869 [00:00<00:00, 920631.31B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3784656/3784656 [00:00<00:00, 4121771.65B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file = bpemb_en.model_file"
      ],
      "metadata": {
        "id": "ls7heUH3tbmf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(str(model_file))\n",
        "print(sp.EncodeAsPieces(\"unhappiness playing\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDE4cJ0Dszsp",
        "outputId": "5d7ae6da-6d90-45dc-b7e6-ce0b5f7f5d4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‚ñÅun', 'h', 'app', 'iness', '‚ñÅplaying']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üîπ Use Case:\n",
        "\n",
        "* Machine Translation\n",
        "* Text Generation\n",
        "* Named Entity Recognition\n",
        "###üîπ Challenges:\n",
        "\n",
        "* Requires a pre-trained model to learn subwords.\n",
        "*Not always intuitive (splitting words into non-intuitive parts)."
      ],
      "metadata": {
        "id": "g9GhNWn8ttSw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nm5gSoNts8r7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}